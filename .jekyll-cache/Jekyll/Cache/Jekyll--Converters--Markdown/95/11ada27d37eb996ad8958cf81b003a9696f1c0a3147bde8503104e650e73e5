I"Ñ<p>In this short post, we summarize Yueâ€™s talk from May 19th and share some of the resources.</p>

<p>Time series data comes as a sequence of values \(y_1,y_2,\ldots\). The goal of forecasting is to be able to predict the outcome at time \(t\) if all the previous outcomes are known. Currently, there are many techniques for doing this; you can find an excellent summary in <a href="link">this paper</a> (about 50 pages long). Coming back to our goal, we want to find a predictor (function) \(f\) such that \(y_t=f(y_1,\ldots,y_{t-1})\).</p>

<h1 id="recursive-neural-networks">Recursive Neural Networks</h1>

<p>RNN are useful to predict sequential data, they are powerful but training them can be hard. This problem is somehow fixed with the introduction of Lont Short Term Memory. They are powerful, like really powerful. We donâ€™t dwell into the explanations here, but refer the reader to (the excellent) <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colahâ€™s post</a> for an explanation or the more technical <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy Blog</a>. Luckily for us, some people have written wrappers that allow us to create an LSTM network with five lines of code using Keras, to see how check <a href="https://youtu.be/ftMq5ps503w">Sirajâ€™s video</a> on using LSTM to predict the stock market.</p>

<h1 id="ensemble-models">Ensemble models</h1>

<p>The idea is simple; we have some classifiers that by themselves arenâ€™t doing well, maybe together they can do better. This is usually understood as building strong classifiers out of weak classifiers. We can do this in different waysâ€¦</p>

<h3 id="random-forest">Random Forest</h3>

<p>The decision of the majorityâ€¦</p>

<h3 id="boosting">Boosting</h3>

<p>Penalizing bad decisionsâ€¦</p>

<h1 id="why-linear-regression-shouldnt-work-and-what-to-do-about-it">Why linear regression shouldnâ€™t work and what to do about it.</h1>

<p>The data is correlatedâ€¦</p>
:ET